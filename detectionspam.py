# -*- coding: utf-8 -*-
"""DetectionSpam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Jd7547EDXOFLgOhQHb4Lq1AuMESyWQo
"""

import pandas as pd
import numpy as np
df = pd.read_csv("/content/mail_data.csv")
df

#info sur le dataset
df.info()

#afficher un message
df.Message.iloc[23]

#afficher le nombre de spam et de ham
df.Category.value_counts()

#afficher le pourcentage de spam et de ham

df.Category.value_counts(normalize = True)

#numerisation de la categories des messages
def text_to_number(text):
  if text == 'ham':
    return 0
  else:
    return 1

#nouvelle colonne
df['class'] = df['Category'].apply(lambda x : text_to_number(x))
df

#feature enginnering: ajout d'une variables 'text_lenght' qui est la longeur de chaque message

df['text_lenght'] = df['Message'].apply(lambda x: len(x))
df.head()

#creation d'une variable qui donnent le taux de penctuation dans le message
#voici toute les penctuation possible
import string
string.punctuation

#creation d'une variable qui donnent le taux de penctuation dans le message
#la focntion qui calcule le taux de penctuation
def count_punctuation(text):
    count_list = [1 for ch in text if ch in string.punctuation]
    nb_punctuation = sum(count_list)
    total = len(text) - text.count(' ')
    return nb_punctuation/total

#creation d'une variable qui donnent le taux de penctuation dans le message

df['punctuation_rate'] = df['Message'].apply(lambda x : count_punctuation(x))
df

#visualisation
import matplotlib.pyplot as plt
plt.hist(df[df['class'] == 1]['text_lenght'], alpha =   0.5 ,  label = 'spam')
plt.hist(df[df['class'] == 0]['text_lenght'],alpha = 0.5 ,  label = 'ham')
plt.legend(loc = 'upper right')
plt.title('longeur du message')
plt.show()

#visualisation
import matplotlib.pyplot as plt
plt.hist(df[df['class'] == 1]['punctuation_rate'], alpha =   0.5 ,  label = 'spam')
plt.hist(df[df['class'] == 0]['punctuation_rate'],alpha = 0.5 ,  label = 'ham')
plt.legend(loc = 'upper right')
plt.title('taux de ponctuation')
plt.show()

#deviser les données en données de test et données d'entrainement

from sklearn.model_selection import train_test_split
seed = 123
X_train, X_test, Y_train, Y_test = train_test_split(
    df[['Message', 'text_lenght',	'punctuation_rate']],
    df['class'],
    test_size = 0.2,
    random_state = seed
)

#voir la dimension des données d'entrainement
X_train.shape

#voir la dimension des données d'entrainement
Y_train.shape

#voir la dimension des données de test
X_test.shape

#voir la dimension des données de test
Y_test.shape

#afficher les message d'entrainement
X_train

#afficher les categories des données d'entrainement
Y_train

#transformer les messages en matrice numerique

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
X_train_vec = vectorizer.fit_transform(X_train['Message'])
X_test_vec = vectorizer.transform(X_test['Message'])

#afficher la matrice numerique des messages d'entrainement
X_train_vec

vectorizer.get_feature_names_out()

#nombre de mots dans les messages d'entrainemet
len(vectorizer.get_feature_names_out())

#afficher la matrice numerique des messages de test
X_test_vec

#transformation de X_train_vec en un dataframe
X_train_content = pd.DataFrame(
    X_train_vec.toarray(),
    columns = vectorizer.get_feature_names_out()
)

X_train_content.head()

#ajouts des colonnes text_lenght et punctuation_rate
X_train_final = pd.concat(
    [
    X_train[['text_lenght' , 'punctuation_rate']].reset_index(drop = True),
    X_train_content
    ],
    axis = 1
)

X_train_final.head()

#ajouts des colonnes text_lenght et punctuation_rate
X_train_num = X_train[['text_lenght' , 'punctuation_rate']].reset_index(drop = True)

# Equilibrage du dataset avec SMOTE
from imblearn.over_sampling import SMOTE
smote = SMOTE()
X_train_num_resampled, Y_train_resampled = smote.fit_resample(X_train_num, Y_train)

# Concaténation des données numériques équilibrées avec les données de texte
X_train_final = pd.concat(
    [
        X_train_num_resampled,
        X_train_content
    ],
    axis = 1
)

# ... (le reste de votre code)

X_train_final = X_train_final.dropna()
Y_train_resampled = Y_train_resampled[X_train_final.index]

#utiliser random forest pour entrainer le modele

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(random_state = seed)
rf_fitted = rf.fit(X_train_final , Y_train_resampled)

#transformation de X_test_vec en un dataframe
X_test_content = pd.DataFrame(
    X_test_vec.toarray(),
    columns = vectorizer.get_feature_names_out()
)

#ajouts des colonnes text_lenght et punctuation_rate
X_test_final = pd.concat(
    [
    X_test[['text_lenght' , 'punctuation_rate']].reset_index(drop = True),
    X_test_content
    ],
    axis = 1
)

#evaluation du modele sur les données de test

rf_fitted.score(X_test_final , Y_test)

#rapport du model
from sklearn.metrics import classification_report
print(classification_report(Y_test, rf_fitted.predict(X_test_final)))

#validation croisées(sur plusiers données d'entrainement differentes)
from sklearn.model_selection import cross_val_score
scores = cross_val_score(rf, X_train_final, Y_train, cv = 3)
scores

#moyennes des validation croisées
scores.mean()

#la mediane des validation croisées
np.median(scores)

new_comment1 = 'check my video on this link please https://www.youtube.com and go to hell admin'

new_data1 = pd.concat(
    [
        pd.DataFrame({'text_lenght':[len(new_comment1)], 'punctuation_rate':[count_punctuation(new_comment1)]}),
        pd.DataFrame(vectorizer.transform([new_comment1]).toarray(), columns = vectorizer.get_feature_names_out())
    ],
    axis = 1
)

new_data1

#inference
rf_fitted.predict(new_data1)

new_comment2 = 'what a great video'

new_data2 = pd.concat(
    [
        pd.DataFrame({'text_lenght':[len(new_comment2)], 'punctuation_rate':[count_punctuation(new_comment2)]}),
        pd.DataFrame(vectorizer.transform([new_comment2]).toarray(), columns = vectorizer.get_feature_names_out())
    ],
    axis = 1
)

new_data1

#inference
rf_fitted.predict(new_data2)

# Entraînement d'un modèle SVM
from sklearn.svm import SVC
svm_model = SVC(kernel='linear')
svm_model.fit(X_train_final, Y_train_resampled)

# Évaluation du modèle SVM
svm_score = svm_model.score(X_test_final, Y_test)
print("Score du modèle SVM :", svm_score)

# Rapport du modèle SVM
from sklearn.metrics import classification_report
print(classification_report(Y_test, svm_model.predict(X_test_final)))

#validation croisées pour le modele svm
from sklearn.model_selection import cross_val_score
scores_svm = cross_val_score(svm_model, X_train_final, Y_train_resampled, cv=3)
print(scores_svm)
print("Mean of cross-validation scores:", scores_svm.mean())

new_comment = "This is an amazing video, thank you for sharing!"

new_data = pd.concat(
    [
        pd.DataFrame({'text_lenght':[len(new_comment)], 'punctuation_rate':[count_punctuation(new_comment)]}),
        pd.DataFrame(vectorizer.transform([new_comment]).toarray(), columns = vectorizer.get_feature_names_out())
    ],
    axis = 1
)

prediction = svm_model.predict(new_data)
print(prediction)

new_spam_comment = "Click here to win a free iPhone! Limited time offer. http://suspiciouslink.com"

new_spam_data = pd.concat(
    [
        pd.DataFrame({'text_lenght':[len(new_spam_comment)], 'punctuation_rate':[count_punctuation(new_spam_comment)]}),
        pd.DataFrame(vectorizer.transform([new_spam_comment]).toarray(), columns = vectorizer.get_feature_names_out())
    ],
    axis = 1
)

prediction = svm_model.predict(new_spam_data)
print(prediction)